<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Multi-Task Gaze Communication Understanding</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      background: #fdfdfd;
      color: #222;
      line-height: 1.6;
      margin: 0;
      padding: 20px;
    }
    .container {
      max-width: 900px;
      margin: auto;
    }
    h1, h2 {
      color: #333;
    }
    a {
      color: #007acc;
      text-decoration: none;
    }
    .links a {
      display: inline-block;
      margin-right: 15px;
      margin-top: 10px;
    }
    pre {
      background: #f4f4f4;
      padding: 10px;
      overflow-x: auto;
    }
    .header {
      margin-bottom: 40px;
    }
  </style>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>Multi-Task Gaze Communication Understanding</h1>
      <p>
        <strong>Cheng Peng</strong>, Oya Celiktutan  
        <br>Department of Engineering, King‚Äôs College London  
        <br><em>ACM Multimedia (MM ‚Äô25), October 27‚Äì31, 2025, Dublin, Ireland</em>
      </p>
    </div>

    <div class="links">
      üìÑ <a href="paper.pdf" target="_blank">[Paper PDF]</a>
      üíª <a href="https://github.com/PengC98/Multi-Task-Gaze-Communication-Understanding" target="_blank">[Code]</a>
      üìÅ <a href="https://example.com/dataset-download" target="_blank">[Dataset (coming soon)]</a>
    </div>

    <h2>Abstract</h2>
    <p>
      Human gaze communication is complex, comprising atomic-level (e.g. mutual, share, etc.) and event-level (e.g. follow, aversion, etc.) behaviours. Existing approaches fall short of fully modeling these dynamics in videos. We propose a novel multi-task model based on CLIP that jointly predicts both atomic and event-level gaze communication, and also estimates gaze targets. Additionally, we present a new annotated dataset, GP-Static++, with rich gaze labels and target information. Our model achieves state-of-the-art performance across multiple benchmarks.
    </p>

    <h2>BibTeX</h2>
    <pre>
@inproceedings{peng2025multitask,
  title={Multi-Task Gaze Communication Understanding},
  author={Peng, Cheng and Celiktutan, Oya},
  booktitle={Proceedings of the 33rd ACM International Conference on Multimedia},
  year={2025},
  address={Dublin, Ireland},
  publisher={ACM},
  doi={10.1145/3746027.3754724}
}
    </pre>

    <h2>Contact</h2>
    <p>
      üìß Email: <a href="mailto:cheng.2.peng@kcl.ac.uk">cheng.2.peng@kcl.ac.uk</a>  
    </p>
  </div>
</body>
</html>
